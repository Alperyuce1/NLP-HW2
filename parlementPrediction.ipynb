{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: spacy in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (70.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: ollama in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.5)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ollama) (2.10.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Requirement already satisfied: evaluate in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.27.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.11.11)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dreky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# write the list of necessary packages here:\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install scikit-learn\n",
    "!pip install ollama\n",
    "!pip install evaluate\n",
    "!pip install transformers[torch] accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages here:\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sex</th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fi00000</td>\n",
       "      <td>396f7d76936fcdda2f63bffd21da7104</td>\n",
       "      <td>F</td>\n",
       "      <td>Arvoisa rouva puhemies! Haluan myöskin osaltan...</td>\n",
       "      <td>Madam President! I would also like to join in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fi00001</td>\n",
       "      <td>59ca750f13ff9333feaeaf7a5f9538ec</td>\n",
       "      <td>F</td>\n",
       "      <td>Arvoisa rouva puhemies! Myös minä kiitän tästä...</td>\n",
       "      <td>Madam President! I too thank you for this deba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fi00002</td>\n",
       "      <td>59ca750f13ff9333feaeaf7a5f9538ec</td>\n",
       "      <td>F</td>\n",
       "      <td>Arvoisa puhemies! Edustaja Haavisto esitti huo...</td>\n",
       "      <td>Mr President! Mr Haavisto expressed his concer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                           speaker sex  \\\n",
       "0  fi00000  396f7d76936fcdda2f63bffd21da7104   F   \n",
       "1  fi00001  59ca750f13ff9333feaeaf7a5f9538ec   F   \n",
       "2  fi00002  59ca750f13ff9333feaeaf7a5f9538ec   F   \n",
       "\n",
       "                                                text  \\\n",
       "0  Arvoisa rouva puhemies! Haluan myöskin osaltan...   \n",
       "1  Arvoisa rouva puhemies! Myös minä kiitän tästä...   \n",
       "2  Arvoisa puhemies! Edustaja Haavisto esitti huo...   \n",
       "\n",
       "                                             text_en  label  \n",
       "0  Madam President! I would also like to join in ...      1  \n",
       "1  Madam President! I too thank you for this deba...      1  \n",
       "2  Mr President! Mr Haavisto expressed his concer...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sex</th>\n",
       "      <th>text</th>\n",
       "      <th>text_en</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fi03180</td>\n",
       "      <td>2241b20fc844da218ddb7f75768cdc1a</td>\n",
       "      <td>F</td>\n",
       "      <td>Arvoisa puhemies! Kiitoksia Suomen valtuuskunn...</td>\n",
       "      <td>Mr President! Thank you to the chairman of the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fi03181</td>\n",
       "      <td>3c7c426430f1c95d746bbd001e0a561f</td>\n",
       "      <td>M</td>\n",
       "      <td>Arvoisa puhemies! Euroopan neuvosto on merkitt...</td>\n",
       "      <td>Mr President! The Council of Europe is an impo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fi03182</td>\n",
       "      <td>13cfa1824e32176b120f49427b1f5e17</td>\n",
       "      <td>M</td>\n",
       "      <td>Arvoisa puhemies! Todellakin jännittyneen kans...</td>\n",
       "      <td>Mr President! Indeed, because of the tense int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                           speaker sex  \\\n",
       "0  fi03180  2241b20fc844da218ddb7f75768cdc1a   F   \n",
       "1  fi03181  3c7c426430f1c95d746bbd001e0a561f   M   \n",
       "2  fi03182  13cfa1824e32176b120f49427b1f5e17   M   \n",
       "\n",
       "                                                text  \\\n",
       "0  Arvoisa puhemies! Kiitoksia Suomen valtuuskunn...   \n",
       "1  Arvoisa puhemies! Euroopan neuvosto on merkitt...   \n",
       "2  Arvoisa puhemies! Todellakin jännittyneen kans...   \n",
       "\n",
       "                                             text_en  label  \n",
       "0  Mr President! Thank you to the chairman of the...      1  \n",
       "1  Mr President! The Council of Europe is an impo...      1  \n",
       "2  Mr President! Indeed, because of the tense int...      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1179 entries, 0 to 1178\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       1179 non-null   object\n",
      " 1   speaker  1179 non-null   object\n",
      " 2   sex      1179 non-null   object\n",
      " 3   text     1179 non-null   object\n",
      " 4   text_en  1049 non-null   object\n",
      " 5   label    1179 non-null   int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 55.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6111 entries, 0 to 6110\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       6111 non-null   object\n",
      " 1   speaker  6111 non-null   object\n",
      " 2   sex      6111 non-null   object\n",
      " 3   text     6111 non-null   object\n",
      " 4   text_en  5427 non-null   object\n",
      " 5   label    6111 non-null   int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 286.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    3386\n",
       "1    2725\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "dataset_ori= pd.read_csv('orientation-fi-train.tsv',sep='\\t')\n",
    "dataset_pow=pd.read_csv('power-fi-train.tsv',sep='\\t')\n",
    "# checking datasets\n",
    "display(dataset_ori.head(3))\n",
    "display(dataset_pow.head(3))\n",
    "dataset_ori.info()\n",
    "dataset_pow.info()\n",
    "dataset_pow['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    676\n",
       "0    503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ori['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test splits\n",
    "dataset_ori=dataset_ori.dropna(subset=['text_en'])\n",
    "dataset_pow=dataset_pow.dropna(subset=['text_en'])\n",
    "ori_train,ori_test=train_test_split(dataset_ori,test_size=0.05)\n",
    "pow_train,pow_test=train_test_split(dataset_pow,test_size=0.05)\n",
    "ori_test = ori_test.reset_index()\n",
    "pow_test = pow_test.reset_index()\n",
    "ori_train = ori_train.reset_index()\n",
    "pow_train = pow_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9170\n",
      "7092\n",
      "5596\n",
      "5077\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 272 entries, 0 to 271\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   index    272 non-null    int64 \n",
      " 1   id       272 non-null    object\n",
      " 2   speaker  272 non-null    object\n",
      " 3   sex      272 non-null    object\n",
      " 4   text     272 non-null    object\n",
      " 5   text_en  272 non-null    object\n",
      " 6   label    272 non-null    int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 15.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53 entries, 0 to 52\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   index    53 non-null     int64 \n",
      " 1   id       53 non-null     object\n",
      " 2   speaker  53 non-null     object\n",
      " 3   sex      53 non-null     object\n",
      " 4   text     53 non-null     object\n",
      " 5   text_en  53 non-null     object\n",
      " 6   label    53 non-null     int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "print(pow_test['text'].str.len().max())\n",
    "print(pow_test['text_en'].str.len().max())\n",
    "\n",
    "print(ori_test['text'].str.len().max())\n",
    "print(ori_test['text_en'].str.len().max())\n",
    "\n",
    "pow_test.info()\n",
    "ori_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row 0\n",
      "processing row 1\n",
      "processing row 2\n",
      "processing row 3\n",
      "processing row 4\n",
      "processing row 5\n",
      "processing row 6\n",
      "processing row 7\n",
      "processing row 8\n",
      "processing row 9\n",
      "processing row 10\n",
      "processing row 11\n",
      "processing row 12\n",
      "processing row 13\n",
      "processing row 14\n",
      "processing row 15\n",
      "processing row 16\n",
      "processing row 17\n",
      "processing row 18\n",
      "processing row 19\n",
      "processing row 20\n",
      "processing row 21\n",
      "processing row 22\n",
      "processing row 23\n",
      "processing row 24\n",
      "processing row 25\n",
      "processing row 26\n",
      "processing row 27\n",
      "processing row 28\n",
      "processing row 29\n",
      "processing row 30\n",
      "processing row 31\n",
      "processing row 32\n",
      "processing row 33\n",
      "processing row 34\n",
      "processing row 35\n",
      "processing row 36\n",
      "processing row 37\n",
      "processing row 38\n",
      "processing row 39\n",
      "processing row 40\n",
      "processing row 41\n",
      "processing row 42\n",
      "processing row 43\n",
      "processing row 44\n",
      "processing row 45\n",
      "processing row 46\n",
      "processing row 47\n",
      "processing row 48\n",
      "processing row 49\n",
      "processing row 50\n",
      "processing row 51\n",
      "processing row 52\n",
      "accuracy from original text = 0.41509433962264153\n",
      "accuracy from english text = 0.3584905660377358\n",
      "amount of refused to label original lang = 8\n",
      "amount of refused to label eng lang = 15\n",
      "only unrefused accuracy from original text = 0.4888888888888889\n",
      "only unrefused accuracy from english text = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Task 1 zero-shot inference\n",
    "# using a multilingual causal language model as zero-shot manner for political oriantation\n",
    "# llama needs to be up and running\n",
    "\n",
    "originaltext_ori_correct_labels=0\n",
    "engtext_ori_correct_labels=0\n",
    "refused_amount_ing=0\n",
    "refused_amount_org=0\n",
    "\n",
    "for index, row in ori_test.iterrows():\n",
    "  print(f\"processing row {index}\")\n",
    "  response: ChatResponse = chat(model='llama3.2:1b', messages=[\n",
    "      {\n",
    "        'role': 'tool',\n",
    "        'content': ('from finnish text only write \"0\" if political oriantation is left or write \"1\" if political oriantation is right finnish text starts here: '+row['text'])[:4500] + ' . \\n ',\n",
    "      },\n",
    "    ],\n",
    "    options={\"temperature\":0.1})\n",
    "  try:\n",
    "    if(int(response['message']['content'][0])==row['label']):\n",
    "      originaltext_ori_correct_labels+=1\n",
    "  except:\n",
    "    refused_amount_org+=1\n",
    "      \n",
    "  if (pd.isna(row['text_en']) or row['text_en'] == ''):\n",
    "    continue\n",
    "  response: ChatResponse = chat(model='llama3.2:1b', messages=[\n",
    "      {\n",
    "        'role': 'tool',\n",
    "        'content': ('from english text from finnish parliament only write \"0\" if political oriantation is left or write \"1\" if political oriantation is right english text starts here: '+str(row['text_en'])[:4500] + ' . \\n '),\n",
    "      },\n",
    "    ],\n",
    "    options={\"temperature\":0.1})\n",
    "  try:\n",
    "    if(int(response['message']['content'][0])==row['label']):\n",
    "      engtext_ori_correct_labels+=1\n",
    "  except:\n",
    "    refused_amount_ing+=1\n",
    "      \n",
    "      \n",
    "    \n",
    "org_accuracy = originaltext_ori_correct_labels/len(ori_test)\n",
    "eng_accuracy = engtext_ori_correct_labels/len(ori_test)\n",
    "print(f\"accuracy from original text = {org_accuracy}\")\n",
    "print(f\"accuracy from english text = {eng_accuracy}\")\n",
    "\n",
    "print(f\"amount of refused to label original lang = {refused_amount_org}\")\n",
    "print(f\"amount of refused to label eng lang = {refused_amount_ing}\")\n",
    "\n",
    "print(f\"only unrefused accuracy from original text = {originaltext_ori_correct_labels/(len(ori_test)-refused_amount_org)}\")\n",
    "print(f\"only unrefused accuracy from english text = {engtext_ori_correct_labels/(len(ori_test)-refused_amount_ing)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row 0\n",
      "processing row 1\n",
      "processing row 2\n",
      "processing row 3\n",
      "processing row 4\n",
      "processing row 5\n",
      "processing row 6\n",
      "processing row 7\n",
      "processing row 8\n",
      "processing row 9\n",
      "processing row 10\n",
      "processing row 11\n",
      "processing row 12\n",
      "processing row 13\n",
      "processing row 14\n",
      "processing row 15\n",
      "processing row 16\n",
      "processing row 17\n",
      "processing row 18\n",
      "processing row 19\n",
      "processing row 20\n",
      "processing row 21\n",
      "processing row 22\n",
      "processing row 23\n",
      "processing row 24\n",
      "processing row 25\n",
      "processing row 26\n",
      "processing row 27\n",
      "processing row 28\n",
      "processing row 29\n",
      "processing row 30\n",
      "processing row 31\n",
      "processing row 32\n",
      "processing row 33\n",
      "processing row 34\n",
      "processing row 35\n",
      "processing row 36\n",
      "processing row 37\n",
      "processing row 38\n",
      "processing row 39\n",
      "processing row 40\n",
      "processing row 41\n",
      "processing row 42\n",
      "processing row 43\n",
      "processing row 44\n",
      "processing row 45\n",
      "processing row 46\n",
      "processing row 47\n",
      "processing row 48\n",
      "processing row 49\n",
      "processing row 50\n",
      "processing row 51\n",
      "processing row 52\n",
      "processing row 53\n",
      "processing row 54\n",
      "processing row 55\n",
      "processing row 56\n",
      "processing row 57\n",
      "processing row 58\n",
      "processing row 59\n",
      "processing row 60\n",
      "processing row 61\n",
      "processing row 62\n",
      "processing row 63\n",
      "processing row 64\n",
      "processing row 65\n",
      "processing row 66\n",
      "processing row 67\n",
      "processing row 68\n",
      "processing row 69\n",
      "processing row 70\n",
      "processing row 71\n",
      "processing row 72\n",
      "processing row 73\n",
      "processing row 74\n",
      "processing row 75\n",
      "processing row 76\n",
      "processing row 77\n",
      "processing row 78\n",
      "processing row 79\n",
      "processing row 80\n",
      "processing row 81\n",
      "processing row 82\n",
      "processing row 83\n",
      "processing row 84\n",
      "processing row 85\n",
      "processing row 86\n",
      "processing row 87\n",
      "processing row 88\n",
      "processing row 89\n",
      "processing row 90\n",
      "processing row 91\n",
      "processing row 92\n",
      "processing row 93\n",
      "processing row 94\n",
      "processing row 95\n",
      "processing row 96\n",
      "processing row 97\n",
      "processing row 98\n",
      "processing row 99\n",
      "processing row 100\n",
      "processing row 101\n",
      "processing row 102\n",
      "processing row 103\n",
      "processing row 104\n",
      "processing row 105\n",
      "processing row 106\n",
      "processing row 107\n",
      "processing row 108\n",
      "processing row 109\n",
      "processing row 110\n",
      "processing row 111\n",
      "processing row 112\n",
      "processing row 113\n",
      "processing row 114\n",
      "processing row 115\n",
      "processing row 116\n",
      "processing row 117\n",
      "processing row 118\n",
      "processing row 119\n",
      "processing row 120\n",
      "processing row 121\n",
      "processing row 122\n",
      "processing row 123\n",
      "processing row 124\n",
      "processing row 125\n",
      "processing row 126\n",
      "processing row 127\n",
      "processing row 128\n",
      "processing row 129\n",
      "processing row 130\n",
      "processing row 131\n",
      "processing row 132\n",
      "processing row 133\n",
      "processing row 134\n",
      "processing row 135\n",
      "processing row 136\n",
      "processing row 137\n",
      "processing row 138\n",
      "processing row 139\n",
      "processing row 140\n",
      "processing row 141\n",
      "processing row 142\n",
      "processing row 143\n",
      "processing row 144\n",
      "processing row 145\n",
      "processing row 146\n",
      "processing row 147\n",
      "processing row 148\n",
      "processing row 149\n",
      "processing row 150\n",
      "processing row 151\n",
      "processing row 152\n",
      "processing row 153\n",
      "processing row 154\n",
      "processing row 155\n",
      "processing row 156\n",
      "processing row 157\n",
      "processing row 158\n",
      "processing row 159\n",
      "processing row 160\n",
      "processing row 161\n",
      "processing row 162\n",
      "processing row 163\n",
      "processing row 164\n",
      "processing row 165\n",
      "processing row 166\n",
      "processing row 167\n",
      "processing row 168\n",
      "processing row 169\n",
      "processing row 170\n",
      "processing row 171\n",
      "processing row 172\n",
      "processing row 173\n",
      "processing row 174\n",
      "processing row 175\n",
      "processing row 176\n",
      "processing row 177\n",
      "processing row 178\n",
      "processing row 179\n",
      "processing row 180\n",
      "processing row 181\n",
      "processing row 182\n",
      "processing row 183\n",
      "processing row 184\n",
      "processing row 185\n",
      "processing row 186\n",
      "processing row 187\n",
      "processing row 188\n",
      "processing row 189\n",
      "processing row 190\n",
      "processing row 191\n",
      "processing row 192\n",
      "processing row 193\n",
      "processing row 194\n",
      "processing row 195\n",
      "processing row 196\n",
      "processing row 197\n",
      "processing row 198\n",
      "processing row 199\n",
      "processing row 200\n",
      "processing row 201\n",
      "processing row 202\n",
      "processing row 203\n",
      "processing row 204\n",
      "processing row 205\n",
      "processing row 206\n",
      "processing row 207\n",
      "processing row 208\n",
      "processing row 209\n",
      "processing row 210\n",
      "processing row 211\n",
      "processing row 212\n",
      "processing row 213\n",
      "processing row 214\n",
      "processing row 215\n",
      "processing row 216\n",
      "processing row 217\n",
      "processing row 218\n",
      "processing row 219\n",
      "processing row 220\n",
      "processing row 221\n",
      "processing row 222\n",
      "processing row 223\n",
      "processing row 224\n",
      "processing row 225\n",
      "processing row 226\n",
      "processing row 227\n",
      "processing row 228\n",
      "processing row 229\n",
      "processing row 230\n",
      "processing row 231\n",
      "processing row 232\n",
      "processing row 233\n",
      "processing row 234\n",
      "processing row 235\n",
      "processing row 236\n",
      "processing row 237\n",
      "processing row 238\n",
      "processing row 239\n",
      "processing row 240\n",
      "processing row 241\n",
      "processing row 242\n",
      "processing row 243\n",
      "processing row 244\n",
      "processing row 245\n",
      "processing row 246\n",
      "processing row 247\n",
      "processing row 248\n",
      "processing row 249\n",
      "processing row 250\n",
      "processing row 251\n",
      "processing row 252\n",
      "processing row 253\n",
      "processing row 254\n",
      "processing row 255\n",
      "processing row 256\n",
      "processing row 257\n",
      "processing row 258\n",
      "processing row 259\n",
      "processing row 260\n",
      "processing row 261\n",
      "processing row 262\n",
      "processing row 263\n",
      "processing row 264\n",
      "processing row 265\n",
      "processing row 266\n",
      "processing row 267\n",
      "processing row 268\n",
      "processing row 269\n",
      "processing row 270\n",
      "processing row 271\n",
      "accuracy from original text = 0.6102941176470589\n",
      "accuracy from english text = 0.5698529411764706\n",
      "amount of refused to label original lang = 5\n",
      "amount of refused to label eng lang = 21\n",
      "only unrefused accuracy from original text = 0.6217228464419475\n",
      "only unrefused accuracy from english text = 0.6175298804780877\n"
     ]
    }
   ],
   "source": [
    "# Task 2 zero-shot inference\n",
    "# using a multilingual causal language model as zero-shot manner for political power\n",
    "# llama needs to be up and running\n",
    "\n",
    "originaltext_pow_correct_labels=0\n",
    "engtext_pow_correct_labels=0\n",
    "refused_amount_org=0\n",
    "refused_amount_ing=0\n",
    "\n",
    "for index, row in pow_test.iterrows():\n",
    "  print(f\"processing row {index}\")\n",
    "  response: ChatResponse = chat(model='llama3.2:1b', messages=[\n",
    "      {\n",
    "        'role': 'tool',\n",
    "        'content': ('from finnish text only write \"0\" if political party is currently governing (in power) or write \"1\" if political party is in opposition finnish text starts here: '+row['text'][:4500]+ ' . \\n '),\n",
    "      },\n",
    "    ],\n",
    "    options={\"temperature\":0.1,\"timeout\":5})\n",
    "  try:\n",
    "    if(int(response['message']['content'][0])==row['label']):\n",
    "      originaltext_pow_correct_labels+=1\n",
    "  except:\n",
    "    refused_amount_org+=1\n",
    "  \n",
    "  if (pd.isna(row['text_en']) or row['text_en'] == ''):\n",
    "    continue\n",
    "  \n",
    "  response: ChatResponse = chat(model='llama3.2:1b', messages=[\n",
    "      {\n",
    "        'role': 'tool',\n",
    "        'content': ('from english text from finnish parliament only write \"0\" if political party is currently governing (in power) or write \"1\" if political party is in opposition english text starts here: '+str(row['text_en'])[:4500]+ ' . \\n '),\n",
    "      },\n",
    "    ],\n",
    "    options={\"temperature\":0.1 , \"timeout\":5})\n",
    "  try:\n",
    "    if(int(response['message']['content'][0])==row['label']):\n",
    "      engtext_pow_correct_labels+=1\n",
    "  except:\n",
    "    refused_amount_ing+=1\n",
    "      \n",
    "      \n",
    "    \n",
    "org_accuracy=originaltext_pow_correct_labels/len(pow_test)\n",
    "eng_accuracy=engtext_pow_correct_labels/len(pow_test)\n",
    "print(f\"accuracy from original text = {org_accuracy}\")\n",
    "print(f\"accuracy from english text = {eng_accuracy}\")\n",
    "\n",
    "print(f\"amount of refused to label original lang = {refused_amount_org}\")\n",
    "print(f\"amount of refused to label eng lang = {refused_amount_ing}\")\n",
    "\n",
    "print(f\"only unrefused accuracy from original text = {originaltext_pow_correct_labels/(len(pow_test)-refused_amount_org)}\")\n",
    "print(f\"only unrefused accuracy from english text = {engtext_pow_correct_labels/(len(pow_test)-refused_amount_ing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# loading the pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-multilingual-cased\", num_labels=2, torch_dtype=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34af2cc2a420444a9622cb08d2ee2234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fcc0f446fe484fa736f8c8c761c84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dreky\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3107837b3bc142ef9a18fdfab69a3109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63754a4e0064837b52b6d182890425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6741549372673035, 'eval_accuracy': 0.5932203389830508, 'eval_runtime': 4.2339, 'eval_samples_per_second': 13.935, 'eval_steps_per_second': 0.472, 'epoch': 1.0}\n",
      "{'loss': 0.6438, 'grad_norm': 11.96977710723877, 'learning_rate': 5.940000000000001e-06, 'epoch': 1.43}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3ab0dbc8a34a62901603af9e0169fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7149451971054077, 'eval_accuracy': 0.6440677966101694, 'eval_runtime': 4.2149, 'eval_samples_per_second': 13.998, 'eval_steps_per_second': 0.475, 'epoch': 2.0}\n",
      "{'loss': 0.4931, 'grad_norm': 34.011714935302734, 'learning_rate': 1.1880000000000001e-05, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2dfc366bbd445f8c2c4db06a37acd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7174217104911804, 'eval_accuracy': 0.5423728813559322, 'eval_runtime': 4.2195, 'eval_samples_per_second': 13.983, 'eval_steps_per_second': 0.474, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a990eb5fe942e4a74a6f613e4e6954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6930614113807678, 'eval_accuracy': 0.576271186440678, 'eval_runtime': 4.2286, 'eval_samples_per_second': 13.953, 'eval_steps_per_second': 0.473, 'epoch': 4.0}\n",
      "{'loss': 0.5842, 'grad_norm': 31.51313591003418, 'learning_rate': 1.7879999999999998e-05, 'epoch': 4.29}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6470033ce84aa1a95859412cccf1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8348823189735413, 'eval_accuracy': 0.5254237288135594, 'eval_runtime': 4.2942, 'eval_samples_per_second': 13.739, 'eval_steps_per_second': 0.466, 'epoch': 5.0}\n",
      "{'loss': 0.5035, 'grad_norm': 46.9262809753418, 'learning_rate': 2.3880000000000002e-05, 'epoch': 5.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdc4ad58fc946709d9aaa80bd1c374b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.926078736782074, 'eval_accuracy': 0.6101694915254238, 'eval_runtime': 4.1895, 'eval_samples_per_second': 14.083, 'eval_steps_per_second': 0.477, 'epoch': 6.0}\n",
      "{'train_runtime': 1637.7192, 'train_samples_per_second': 4.103, 'train_steps_per_second': 0.256, 'train_loss': 0.5486969448271252, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=420, training_loss=0.5486969448271252, metrics={'train_runtime': 1637.7192, 'train_samples_per_second': 4.103, 'train_steps_per_second': 0.256, 'total_flos': 442026573004800.0, 'train_loss': 0.5486969448271252, 'epoch': 6.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making trainer for political oriantation it uses original language\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized[\"labels\"] = examples[\"label\"]  # Ensure labels are included\n",
    "    return tokenized\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "train_dataset = ori_train[[\"text\", \"label\"]]  # Keep only relevant columns\n",
    "test_dataset = ori_test[[\"text\", \"label\"]]\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "\n",
    "tokenized_ori_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_ori_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",             # Directory to save model and checkpoints\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate after every epoch\n",
    "    per_device_train_batch_size=16,       # Train on 16 samples per GPU per batch\n",
    "    per_device_eval_batch_size=32,        # Use a larger batch size for evaluation\n",
    "    num_train_epochs=6,                   # Train for 6 epochs (adjust as needed)\n",
    "    learning_rate=3e-5,                   # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,                    # Regularization to prevent overfitting\n",
    "    warmup_steps=500,                     # Warmup for LR scheduling\n",
    "    logging_dir=\"logs\",                   # Directory for logs\n",
    "    logging_steps=100,                    # Log training metrics every 100 steps\n",
    "    save_steps=500,                       # Save model every 500 steps\n",
    "    save_total_limit=2,                   # Keep only the last 2 checkpoints\n",
    "    fp16=True,                            # Use mixed-precision training\n",
    "    report_to=\"none\",                     # Disable reporting to external tools\n",
    "    disable_tqdm=False,                   # Show progress bar\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ori_train,\n",
    "    eval_dataset=tokenized_ori_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3547dc25a894c7e80c211807e8172fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6930655241012573, 'eval_accuracy': 0.5423728813559322, 'eval_runtime': 4.2504, 'eval_samples_per_second': 13.881, 'eval_steps_per_second': 0.471, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "output copied:{'eval_loss': 0.6105232834815979, 'eval_accuracy': 0.711864406779661, 'eval_runtime': 16.8895, 'eval_samples_per_second': 3.493, 'eval_steps_per_second': 0.118, 'epoch': 3.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fe28ffd22044f6a2d2bdb8c0e5fae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8961076d76bf4d65a307bb0f1782492e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dreky\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b590331932324045be659544d5d99171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1932 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7044, 'grad_norm': 15.2473783493042, 'learning_rate': 5.76e-06, 'epoch': 0.31}\n",
      "{'loss': 0.6948, 'grad_norm': 1.6054680347442627, 'learning_rate': 1.1760000000000001e-05, 'epoch': 0.62}\n",
      "{'loss': 0.6889, 'grad_norm': 1.146223783493042, 'learning_rate': 1.776e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2809d6cb12b40b0a8ee27596a837e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6643297076225281, 'eval_accuracy': 0.5818181818181818, 'eval_runtime': 20.2062, 'eval_samples_per_second': 13.61, 'eval_steps_per_second': 0.445, 'epoch': 1.0}\n",
      "{'loss': 0.6834, 'grad_norm': 4.863689422607422, 'learning_rate': 2.3760000000000003e-05, 'epoch': 1.24}\n",
      "{'loss': 0.6842, 'grad_norm': 1.3825929164886475, 'learning_rate': 2.976e-05, 'epoch': 1.55}\n",
      "{'loss': 0.6859, 'grad_norm': 2.064964532852173, 'learning_rate': 2.8009776536312848e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c4b1f242d8431bb757505cec7ef25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6571146845817566, 'eval_accuracy': 0.6327272727272727, 'eval_runtime': 20.0793, 'eval_samples_per_second': 13.696, 'eval_steps_per_second': 0.448, 'epoch': 2.0}\n",
      "{'loss': 0.6673, 'grad_norm': 5.223665237426758, 'learning_rate': 2.5914804469273745e-05, 'epoch': 2.17}\n",
      "{'loss': 0.6208, 'grad_norm': 3.3124122619628906, 'learning_rate': 2.3819832402234635e-05, 'epoch': 2.48}\n",
      "{'loss': 0.6183, 'grad_norm': 4.197593688964844, 'learning_rate': 2.172486033519553e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0574431e9be46e080da730f6de6b615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6664601564407349, 'eval_accuracy': 0.64, 'eval_runtime': 20.1005, 'eval_samples_per_second': 13.681, 'eval_steps_per_second': 0.448, 'epoch': 3.0}\n",
      "{'loss': 0.5959, 'grad_norm': 12.572427749633789, 'learning_rate': 1.9629888268156425e-05, 'epoch': 3.11}\n",
      "{'loss': 0.4852, 'grad_norm': 11.710397720336914, 'learning_rate': 1.753491620111732e-05, 'epoch': 3.42}\n",
      "{'loss': 0.4799, 'grad_norm': 14.270332336425781, 'learning_rate': 1.5439944134078212e-05, 'epoch': 3.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15bb72ba0c264fa1b53e4c759114fd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8096352219581604, 'eval_accuracy': 0.6472727272727272, 'eval_runtime': 20.1548, 'eval_samples_per_second': 13.644, 'eval_steps_per_second': 0.447, 'epoch': 4.0}\n",
      "{'loss': 0.425, 'grad_norm': 4.116805553436279, 'learning_rate': 1.3344972067039107e-05, 'epoch': 4.04}\n",
      "{'loss': 0.308, 'grad_norm': 15.36822509765625, 'learning_rate': 1.125e-05, 'epoch': 4.35}\n",
      "{'loss': 0.275, 'grad_norm': 15.533489227294922, 'learning_rate': 9.155027932960894e-06, 'epoch': 4.66}\n",
      "{'loss': 0.2917, 'grad_norm': 18.683624267578125, 'learning_rate': 7.0600558659217885e-06, 'epoch': 4.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d5039efcea464f90fe97adc8d41645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1395609378814697, 'eval_accuracy': 0.6218181818181818, 'eval_runtime': 20.0829, 'eval_samples_per_second': 13.693, 'eval_steps_per_second': 0.448, 'epoch': 5.0}\n",
      "{'loss': 0.1826, 'grad_norm': 25.390962600708008, 'learning_rate': 4.965083798882682e-06, 'epoch': 5.28}\n",
      "{'loss': 0.1665, 'grad_norm': 72.68968963623047, 'learning_rate': 2.8701117318435755e-06, 'epoch': 5.59}\n",
      "{'loss': 0.1491, 'grad_norm': 32.206302642822266, 'learning_rate': 7.751396648044693e-07, 'epoch': 5.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d8437b7a6643d28c8b8478ea5a88a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5836676359176636, 'eval_accuracy': 0.6327272727272727, 'eval_runtime': 19.5034, 'eval_samples_per_second': 14.1, 'eval_steps_per_second': 0.461, 'epoch': 6.0}\n",
      "{'train_runtime': 9199.827, 'train_samples_per_second': 3.36, 'train_steps_per_second': 0.21, 'train_loss': 0.49041426724775483, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1932, training_loss=0.49041426724775483, metrics={'train_runtime': 9199.827, 'train_samples_per_second': 3.36, 'train_steps_per_second': 0.21, 'total_flos': 2033322235822080.0, 'train_loss': 0.49041426724775483, 'epoch': 6.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making trainer for power it uses english\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-cased\")\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-multilingual-cased\", num_labels=2, torch_dtype=\"auto\")\n",
    "\n",
    "def tokenize_function2(examples):\n",
    "    tokenized = tokenizer(examples[\"text_en\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized[\"labels\"] = examples[\"label\"]  # Ensure labels are included\n",
    "    return tokenized\n",
    "\n",
    "metric2 = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics2(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric2.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "train_dataset2 = pow_train[[\"text_en\", \"label\"]]  # Keep only relevant columns\n",
    "test_dataset2 = pow_test[[\"text_en\", \"label\"]]\n",
    "\n",
    "train_dataset2 = train_dataset2.dropna(subset=[\"text_en\"]) # delete untranslated ones  \n",
    "test_dataset2 = test_dataset2.dropna(subset=[\"text_en\"])\n",
    "\n",
    "train_dataset2 = Dataset.from_pandas(train_dataset2)\n",
    "test_dataset2 = Dataset.from_pandas(test_dataset2)\n",
    "\n",
    "tokenized_pow_train = train_dataset2.map(tokenize_function2, batched=True)\n",
    "tokenized_pow_test = test_dataset2.map(tokenize_function2, batched=True)\n",
    "\n",
    "\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=\"test_trainer2\",             # Directory to save model and checkpoints\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate after every epoch\n",
    "    per_device_train_batch_size=16,       # Train on 16 samples per GPU per batch\n",
    "    per_device_eval_batch_size=32,        # Use a larger batch size for evaluation\n",
    "    num_train_epochs=6,                   # Train for 3 epochs (adjust as needed)\n",
    "    learning_rate=3e-5,                   # Lower learning rate for fine-tuning\n",
    "    weight_decay=0.01,                    # Regularization to prevent overfitting\n",
    "    warmup_steps=500,                     # Warmup for LR scheduling\n",
    "    logging_dir=\"logs\",                   # Directory for logs\n",
    "    logging_steps=100,                    # Log training metrics every 100 steps\n",
    "    save_steps=500,                       # Save model every 500 steps\n",
    "    save_total_limit=2,                   # Keep only the last 2 checkpoints\n",
    "    fp16=True,                            # Use mixed-precision training\n",
    "    report_to=\"none\",                     # Disable reporting to external tools\n",
    "    disable_tqdm=False,                   # Show progress bar\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=tokenized_pow_train,\n",
    "    eval_dataset=tokenized_pow_test,\n",
    "    compute_metrics=compute_metrics2,\n",
    ")\n",
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453d7e933dd145c6a07e767670676111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5836676359176636, 'eval_accuracy': 0.6327272727272727, 'eval_runtime': 19.6338, 'eval_samples_per_second': 14.006, 'eval_steps_per_second': 0.458, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results2 = trainer2.evaluate()\n",
    "print(eval_results2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
